--- a/arch/arm64/crypto/aes-neonbs-glue.c
+++ b/arch/arm64/crypto/aes-neonbs-glue.c
@@ -239,13 +239,29 @@ static int ctr_encrypt(struct skcipher_r
 		u8 *final = (walk.total % AES_BLOCK_SIZE) ? buf : NULL;
 
 		if (walk.nbytes < walk.total) {
-			blocks = round_down(blocks,
-					    walk.stride / AES_BLOCK_SIZE);
+			blocks = round_down(blocks, walk.stride / AES_BLOCK_SIZE);
 			final = NULL;
 		}
 
-		aesbs_ctr_encrypt(walk.dst.virt.addr, walk.src.virt.addr,
-				  ctx->rk, ctx->rounds, blocks, walk.iv, final);
+		// Check if the remaining bytes are less than a block size
+		if (unlikely(walk.nbytes < AES_BLOCK_SIZE)) {
+			// Copy the remaining bytes to the temporary buffer
+			u8 *src = walk.src.virt.addr;
+			u8 *dst = walk.dst.virt.addr;
+			memcpy(buf + sizeof(buf) - walk.nbytes, src, walk.nbytes);
+
+			// Perform encryption using the temporary buffer
+			aesbs_ctr_encrypt(buf, buf, ctx->rk, ctx->rounds, 1, walk.iv, buf);
+
+			// Copy the encrypted data back to the destination
+			memcpy(dst, buf + sizeof(buf) - walk.nbytes, walk.nbytes);
+
+			// Complete the walk
+			err = skcipher_walk_done(&walk, 0);
+			break;
+		}
+		
+		aesbs_ctr_encrypt(walk.dst.virt.addr, walk.src.virt.addr, ctx->rk, ctx->rounds, blocks, walk.iv, final);
 
 		if (final) {
 			u8 *dst = walk.dst.virt.addr + blocks * AES_BLOCK_SIZE;
@@ -257,8 +273,7 @@ static int ctr_encrypt(struct skcipher_r
 			err = skcipher_walk_done(&walk, 0);
 			break;
 		}
-		err = skcipher_walk_done(&walk,
-					 walk.nbytes - blocks * AES_BLOCK_SIZE);
+		err = skcipher_walk_done(&walk, walk.nbytes - blocks * AES_BLOCK_SIZE);
 	}
 	kernel_neon_end();
 
